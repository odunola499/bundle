# Model Configuration
model:
  name: "Transformer"
  model_type: "encoder-decoder"  # Can be "encoder-decoder", "encoder", or "decoder"
  vocab_size: 10000
  dim_model: 512

  # Positional Encoding
  positional_encoding:
    name: "absolute"  # "absolute" or "rope"
    max_seq_len: 512
    rope_theta: 10000.0

  # Encoder Configuration (only used if model_type is "encoder" or "encoder-decoder")
  encoder:
    num_layers: 6
    attention:
      name: "MHA"  # MHA, MQA, MLA, EfficientMLA
      num_heads: 8
      dim_k: 64
      dim_v: 64
      is_causal: False
      latent_dim: 128  # For MLA
      low_dim: 64  # For EfficientMLA
      num_group_heads: 8 #for GQA
    ff:
      name: "StandardFeedForward"
      dim_ff: 2048
    dropout: 0.1

  # Decoder Configuration (only used if model_type is "decoder" or "encoder-decoder")
  decoder:
    num_layers: 6
    attention:
      name: "MHA"  # MHA, MQA, MLA, EfficientMLA
      num_heads: 8
      dim_k: 64
      dim_v: 64
      is_causal: True
      latent_dim: 128  # For MLA
      low_dim: 64  # For EfficientMLA
    ff:
      name: "StandardFeedForward"
      dim_ff: 2048
    dropout: 0.1
