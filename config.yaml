model:
  name: "Decoder" #can be one of "Encoder", "Decoder" or "Encoder-Decoder"


  positional_encoding:
    name: "absolute" #can be one of "absolute, "relative" or "rotational"
    dim_model: ${model.encoder.constants.dropout}
    max_seq_length: 512

  encoder:
    constants:
      vocab_size: 10000
      dim_model: 512
      max_seq_length: ${model.positional_encoding.max_seq_length}
      dropout: 0.1

    attention:
      name: "MHA" #can be one of MQA, MLA, MHA, GQA, SWA
      num_heads: 8
      dim_model: ${model.encoder.constants.dim_model}
      dim_k: 32
      dim_v: 32
      dropout: ${model.encoder.constants.dropout}
      num_query_heads: 4  #this is only functional for GQA, value has to be less than num_hheads
      latent_dim: 64 #only useful for MLA
      window_size: 32 # this is only functional for SWA,
      rope_dim: 16 # should be less than dim_k
    ff:
      name: "FeedForward"

  decoder:
    attention:
      name: "MHA" #can be one of MQA, MLA, MHA
      num_heads: 8
      dim_model: 512
      dim_k: 32
      dim_v: 32
      dropout: 0.1
    ff:
      name: "FeedForward"

